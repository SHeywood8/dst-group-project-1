{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb734ab-c1b4-4620-8eae-296138c7cab2",
   "metadata": {},
   "source": [
    "# Attempts At Improvement\n",
    "\n",
    "In this document we attempt a few methods at improving the performance of the Logistic Regression model. These methods are as follows:\n",
    "\n",
    "- Modifying the C Value of the model\n",
    "- Dealing With Missing Data\n",
    "- Cross Validation\n",
    "\n",
    "We first get our data back into an analysable state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86769836-b3c7-42cc-b5f5-1e83515a01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt # This can be used to install the necessary modules if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf58d8a4-838f-4975-8529-caef0f6400a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5ed0b5-1d52-4e63-a3d0-3fbc19e00fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../data/X_train.csv\", index_col=0)  # Use the first column as index\n",
    "y_train = pd.read_csv(\"../data/y_train.csv\", index_col=0)  # Use the first column as index\n",
    "X_test = pd.read_csv(\"../data/X_test.csv\", index_col=0)    # Use the first column as index\n",
    "y_test = pd.read_csv(\"../data/y_test.csv\", index_col=0)    # Use the first column as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e72da5-d3e9-4296-8579-e9093029dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pd.concat({'X_train':X_train, 'X_test':X_test})\n",
    "objects = ['workclass','education','marital-status','occupation','relationship','race','sex','native-country']\n",
    "keys = [0]*len(objects)\n",
    "\n",
    "for i in range(len(objects)):\n",
    "    X_all[objects[i]], keys[i] = pd.factorize(X_all[objects[i]])\n",
    "\n",
    "X_train = X_all.loc['X_train']\n",
    "X_test = X_all.loc['X_test']\n",
    "\n",
    "y_all = pd.concat({'y_train':y_train, 'y_test':y_test})\n",
    "y_all['income'], income_key = pd.factorize(y_all['income'])\n",
    "\n",
    "y_train = y_all.loc['y_train']\n",
    "y_test = y_all.loc['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f11c524-09d7-43c7-8cac-41958e7347a2",
   "metadata": {},
   "source": [
    "## Modifying the C Value of the model\n",
    "\n",
    "The LogisticRegression function in sklearn has a parameter known simply as C, set to C=1 by default. This parameter represents the inverse of the regularisation strength, where smaller values represent stronger regularisation. In our case, the training and testing scores are fairly similar and so we don't need to worry about potential overfitting, but we may have some underfitting. We try a few values of C, higher values produce a more flexible model, whilst lower values produce a more regularised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d110a73-954c-4fd3-88bb-c3f7167b9b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training score with C=0.01: 0.825942\n",
      "Our training score with C=1:    0.825891\n",
      "Our training score with C=100:  0.825866\n",
      "Our testing score with C=0.01:  0.820357\n",
      "Our testing score with C=1:     0.820137\n",
      "Our testing score with C=100:   0.820247\n"
     ]
    }
   ],
   "source": [
    "pipe001 = make_pipeline(StandardScaler(), linear_model.LogisticRegression(C=0.1))\n",
    "pipe = make_pipeline(StandardScaler(), linear_model.LogisticRegression())\n",
    "pipe100 = make_pipeline(StandardScaler(), linear_model.LogisticRegression(C=10))\n",
    "\n",
    "pipe001.fit(X_train, y_train.values.ravel())\n",
    "pipe.fit(X_train, y_train.values.ravel())\n",
    "pipe100.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "#y_pred001 = pipe001.predict(X_test)\n",
    "#y_pred100 = pipe100.predict(X_test)\n",
    "\n",
    "trainscore001 = pipe001.score(X_train, y_train)\n",
    "trainscore = pipe.score(X_train, y_train)\n",
    "trainscore100 = pipe100.score(X_train, y_train)\n",
    "\n",
    "testscore001 = pipe001.score(X_test, y_test)\n",
    "testscore = pipe.score(X_test, y_test)\n",
    "testscore100 = pipe100.score(X_test, y_test)\n",
    "\n",
    "print('Our training score with C=0.01: {0:0.6f}'.format(trainscore001))\n",
    "print('Our training score with C=1:    {0:0.6f}'.format(trainscore))\n",
    "print('Our training score with C=100:  {0:0.6f}'.format(trainscore100))\n",
    "print('Our testing score with C=0.01:  {0:0.6f}'.format(testscore001))\n",
    "print('Our testing score with C=1:     {0:0.6f}'.format(testscore))\n",
    "print('Our testing score with C=100:   {0:0.6f}'.format(testscore100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891fc6d-539e-4cda-abc3-d3ac49be61b1",
   "metadata": {},
   "source": [
    "We see that for the training data, C=1 remain the best value, but both of the other values for the testing data provide a slight improvement over the default, with C=0.01 having a larger increase. However, this increase is not largely notable. It does tell us that the default value of C was not optimal though, and as such our model could be refined if we desired that small increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132f036-1d90-41be-9970-761b82cc989c",
   "metadata": {},
   "source": [
    "## Dealing With Missing Data\n",
    "\n",
    "Another thing that may be affecting our model is the missing data. Our data only contains missing entries in the training data and none in the testing data. This is good since we wont be surprised with missing values when testing, but also bad since these missing values could be affecting the efficiency of the model. We will test a couple different methods, namely removing the rows with missing data, or methods of imputing the missing data.\n",
    "\n",
    "First we check to see where the missing data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2b7f21-97ef-4171-b954-7974ea952401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         2799\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education-num        0\n",
       "marital-status       0\n",
       "occupation        2809\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital-gain         0\n",
       "capital-loss         0\n",
       "hours-per-week       0\n",
       "native-country     857\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2 = pd.read_csv(\"../data/X_train.csv\", index_col=0) # Reimport dataset so we have it in it's original state\n",
    "X_train2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417ad14-7429-4af3-a7d2-3ef03afac2be",
   "metadata": {},
   "source": [
    "There are 3 columns with missing data, namely 'workclass', 'occupation', and 'native-country'. These are all originally object features, and so during our factorising, the NaN values would have been converted to '-1', which we can see below. This could cause problems for our data, especially when we scaled it as the mean and variance will be skewed by these values, and so this motivates us to investigate how fixing this problem will impact our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bbdc5f1-745f-4cf9-8b1f-653253f4f82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4 -1  5  6  7]\n"
     ]
    }
   ],
   "source": [
    "print(X_train['workclass'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf569108-6bec-4507-99d5-17eb1111fa45",
   "metadata": {},
   "source": [
    "One easy way to deal with the data is to simply **remove the rows with missing values**. This can be dangerous for smaller datasets but since for our group the number of rows with missing data is around 10%, this could be a valid way to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be051da-7ed6-4c99-8a23-b0911fc47478",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingcols = ['workclass','occupation','native-country'] # collect missing columns\n",
    "X_train_nan = X_train.copy() # We no longer want to alter the original dataframe so we make a copy for our tests\n",
    "\n",
    "# convert the missing values from -1 back to nan, so that we can then use the .dropna method\n",
    "for col in missingcols:\n",
    "    X_train_nan.loc[X_train[col] == -1,col] = np.nan\n",
    "\n",
    "X_y_All = pd.concat({'X_train':X_train_nan,'y_train':y_train},axis=1)\n",
    "X_y_All = X_y_All.dropna()\n",
    "\n",
    "X_train2 = X_y_All.loc[:,'X_train']\n",
    "y_train2 = X_y_All.loc[:,'y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97e2950-8f37-4583-b754-df665b087184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training score with removed rows: 0.8268466837632624\n",
      "The training score without removed rows: 0.8258912857645698\n",
      "\n",
      "The test score with removed rows: 0.8198059108954565\n",
      "The test score without removed rows: 0.8201367445963829\n"
     ]
    }
   ],
   "source": [
    "pipe_rem = make_pipeline(StandardScaler(), linear_model.LogisticRegression())\n",
    "pipe_rem.fit(X_train2, y_train2.values.ravel())\n",
    "\n",
    "trainscore_rem = pipe_rem.score(X_train, y_train) # Using the model trained on the removed rows back on the dataset with missing values\n",
    "testscore_rem = pipe_rem.score(X_test, y_test)\n",
    "print(f'The training score with removed rows: {trainscore_rem}')\n",
    "print(f'The training score without removed rows: {trainscore}\\n')\n",
    "print(f'The test score with removed rows: {testscore_rem}')\n",
    "print(f'The test score without removed rows: {testscore}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0ee36-3b86-478b-a2a4-f36578343dda",
   "metadata": {},
   "source": [
    "Unfortunately we see that removing the rows has infact decreased our score. This suggests that the missing data does not have much of an impact on our performance, and the reason that our score suffered after removing the rows was due to the decreased amount of data. This reassures us that the missing data is not having a negative affect on our models performance, and discourages us from attempting further methods of dealing with missing data such as imputing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88c9041-25b3-48a3-8c4f-cd0a8d4ec0a2",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "The third and final method of improvement that we will try is using cross-validation, specifically K-fold Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb8231cc-fefe-4387-b9ef-9da4ecf37d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score from our initial model: 0.8259\n",
      "Average cross-validation score: 0.8257 for cv = 2\n",
      "Average cross-validation score: 0.8259 for cv = 5\n",
      "Average cross-validation score: 0.8254 for cv = 10\n",
      "Average cross-validation score: 0.8257 for cv = 25\n",
      "Average cross-validation score: 0.8257 for cv = 50\n"
     ]
    }
   ],
   "source": [
    "print('The score from our initial model: {:.4f}'.format(trainscore))\n",
    "\n",
    "for x in [2,5,10,25,50]:\n",
    "    scores = cross_val_score(pipe, X_train, y_train.values.ravel(), cv = x, scoring='accuracy')\n",
    "    print('Average cross-validation score: {:.4f} for cv = {}'.format(scores.mean(),x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4ba2c-eb37-47a5-806a-d9987e24321e",
   "metadata": {},
   "source": [
    "We see that yet again, this method does not improve our score, and in fact all tests are the same or lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8bd32e-95f5-4c1d-be57-418b5f96e876",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Thus, in conclusion we have that this model does not seem to be easily improvable, and our initial score was quite good for the limitations of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
